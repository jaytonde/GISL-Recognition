{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#imports\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport json","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-25T16:11:55.595283Z","iopub.execute_input":"2023-04-25T16:11:55.596078Z","iopub.status.idle":"2023-04-25T16:11:55.601294Z","shell.execute_reply.started":"2023-04-25T16:11:55.596038Z","shell.execute_reply":"2023-04-25T16:11:55.600120Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"!pip install tslearn plotly","metadata":{"execution":{"iopub.status.busy":"2023-04-25T14:25:24.168088Z","iopub.execute_input":"2023-04-25T14:25:24.168472Z","iopub.status.idle":"2023-04-25T14:25:34.599942Z","shell.execute_reply.started":"2023-04-25T14:25:24.168427Z","shell.execute_reply":"2023-04-25T14:25:34.598730Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Reading train.csv file which describes data**","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/asl-signs/train.csv\")\nprint(df_train.shape)\ndf_train.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T16:12:00.545771Z","iopub.execute_input":"2023-04-25T16:12:00.546208Z","iopub.status.idle":"2023-04-25T16:12:00.802899Z","shell.execute_reply.started":"2023-04-25T16:12:00.546169Z","shell.execute_reply":"2023-04-25T16:12:00.801641Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"(94477, 4)\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                            path  participant_id  sequence_id  \\\n0  train_landmark_files/26734/1000035562.parquet           26734   1000035562   \n1  train_landmark_files/28656/1000106739.parquet           28656   1000106739   \n2   train_landmark_files/16069/100015657.parquet           16069    100015657   \n3  train_landmark_files/25571/1000210073.parquet           25571   1000210073   \n4  train_landmark_files/62590/1000240708.parquet           62590   1000240708   \n5  train_landmark_files/26734/1000241583.parquet           26734   1000241583   \n6  train_landmark_files/26734/1000255522.parquet           26734   1000255522   \n7  train_landmark_files/32319/1000278229.parquet           32319   1000278229   \n8   train_landmark_files/37055/100035691.parquet           37055    100035691   \n9   train_landmark_files/29302/100039661.parquet           29302    100039661   \n\n     sign  \n0    blow  \n1    wait  \n2   cloud  \n3    bird  \n4    owie  \n5    duck  \n6  minemy  \n7    lips  \n8  flower  \n9    time  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>participant_id</th>\n      <th>sequence_id</th>\n      <th>sign</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_landmark_files/26734/1000035562.parquet</td>\n      <td>26734</td>\n      <td>1000035562</td>\n      <td>blow</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_landmark_files/28656/1000106739.parquet</td>\n      <td>28656</td>\n      <td>1000106739</td>\n      <td>wait</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_landmark_files/16069/100015657.parquet</td>\n      <td>16069</td>\n      <td>100015657</td>\n      <td>cloud</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_landmark_files/25571/1000210073.parquet</td>\n      <td>25571</td>\n      <td>1000210073</td>\n      <td>bird</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_landmark_files/62590/1000240708.parquet</td>\n      <td>62590</td>\n      <td>1000240708</td>\n      <td>owie</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>train_landmark_files/26734/1000241583.parquet</td>\n      <td>26734</td>\n      <td>1000241583</td>\n      <td>duck</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>train_landmark_files/26734/1000255522.parquet</td>\n      <td>26734</td>\n      <td>1000255522</td>\n      <td>minemy</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>train_landmark_files/32319/1000278229.parquet</td>\n      <td>32319</td>\n      <td>1000278229</td>\n      <td>lips</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>train_landmark_files/37055/100035691.parquet</td>\n      <td>37055</td>\n      <td>100035691</td>\n      <td>flower</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>train_landmark_files/29302/100039661.parquet</td>\n      <td>29302</td>\n      <td>100039661</td>\n      <td>time</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"**Reading sign_to_prediction_index_map.json which contains integer label for each sign**","metadata":{}},{"cell_type":"code","source":"json_file_path = \"/kaggle/input/asl-signs/sign_to_prediction_index_map.json\"\nwith open(json_file_path, 'r') as j:\n     sign_dict = json.loads(j.read())\n        \nordered_signs = list(sign_dict.keys())\nprint(ordered_signs)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T16:12:54.748083Z","iopub.execute_input":"2023-04-25T16:12:54.748563Z","iopub.status.idle":"2023-04-25T16:12:54.761154Z","shell.execute_reply.started":"2023-04-25T16:12:54.748525Z","shell.execute_reply":"2023-04-25T16:12:54.759567Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"['TV', 'after', 'airplane', 'all', 'alligator', 'animal', 'another', 'any', 'apple', 'arm', 'aunt', 'awake', 'backyard', 'bad', 'balloon', 'bath', 'because', 'bed', 'bedroom', 'bee', 'before', 'beside', 'better', 'bird', 'black', 'blow', 'blue', 'boat', 'book', 'boy', 'brother', 'brown', 'bug', 'bye', 'callonphone', 'can', 'car', 'carrot', 'cat', 'cereal', 'chair', 'cheek', 'child', 'chin', 'chocolate', 'clean', 'close', 'closet', 'cloud', 'clown', 'cow', 'cowboy', 'cry', 'cut', 'cute', 'dad', 'dance', 'dirty', 'dog', 'doll', 'donkey', 'down', 'drawer', 'drink', 'drop', 'dry', 'dryer', 'duck', 'ear', 'elephant', 'empty', 'every', 'eye', 'face', 'fall', 'farm', 'fast', 'feet', 'find', 'fine', 'finger', 'finish', 'fireman', 'first', 'fish', 'flag', 'flower', 'food', 'for', 'frenchfries', 'frog', 'garbage', 'gift', 'giraffe', 'girl', 'give', 'glasswindow', 'go', 'goose', 'grandma', 'grandpa', 'grass', 'green', 'gum', 'hair', 'happy', 'hat', 'hate', 'have', 'haveto', 'head', 'hear', 'helicopter', 'hello', 'hen', 'hesheit', 'hide', 'high', 'home', 'horse', 'hot', 'hungry', 'icecream', 'if', 'into', 'jacket', 'jeans', 'jump', 'kiss', 'kitty', 'lamp', 'later', 'like', 'lion', 'lips', 'listen', 'look', 'loud', 'mad', 'make', 'man', 'many', 'milk', 'minemy', 'mitten', 'mom', 'moon', 'morning', 'mouse', 'mouth', 'nap', 'napkin', 'night', 'no', 'noisy', 'nose', 'not', 'now', 'nuts', 'old', 'on', 'open', 'orange', 'outside', 'owie', 'owl', 'pajamas', 'pen', 'pencil', 'penny', 'person', 'pig', 'pizza', 'please', 'police', 'pool', 'potty', 'pretend', 'pretty', 'puppy', 'puzzle', 'quiet', 'radio', 'rain', 'read', 'red', 'refrigerator', 'ride', 'room', 'sad', 'same', 'say', 'scissors', 'see', 'shhh', 'shirt', 'shoe', 'shower', 'sick', 'sleep', 'sleepy', 'smile', 'snack', 'snow', 'stairs', 'stay', 'sticky', 'store', 'story', 'stuck', 'sun', 'table', 'talk', 'taste', 'thankyou', 'that', 'there', 'think', 'thirsty', 'tiger', 'time', 'tomorrow', 'tongue', 'tooth', 'toothbrush', 'touch', 'toy', 'tree', 'uncle', 'underwear', 'up', 'vacuum', 'wait', 'wake', 'water', 'wet', 'weus', 'where', 'white', 'who', 'why', 'will', 'wolf', 'yellow', 'yes', 'yesterday', 'yourself', 'yucky', 'zebra', 'zipper']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Function to remove null values**","metadata":{}},{"cell_type":"code","source":"def load_relevant_data(pq_path):\n    data = pd.read_parquet(pq_path).fillna(0)\n    return data","metadata":{"execution":{"iopub.status.busy":"2023-04-25T16:20:19.586329Z","iopub.execute_input":"2023-04-25T16:20:19.587026Z","iopub.status.idle":"2023-04-25T16:20:19.592862Z","shell.execute_reply.started":"2023-04-25T16:20:19.586989Z","shell.execute_reply":"2023-04-25T16:20:19.591499Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"**Function to reshape data**","metadata":{}},{"cell_type":"code","source":"ROWS_PER_FRAME = 543  # number of landmarks per frame\ndef load_relevant_data_subset(pq_path):\n    data_columns = ['x', 'y', 'z']\n    data = pd.read_parquet(pq_path, columns=data_columns).fillna(0)\n    n_frames = int(len(data) / ROWS_PER_FRAME)\n    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n    return data.astype(np.float32)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T16:20:23.923044Z","iopub.execute_input":"2023-04-25T16:20:23.923471Z","iopub.status.idle":"2023-04-25T16:20:23.930056Z","shell.execute_reply.started":"2023-04-25T16:20:23.923434Z","shell.execute_reply":"2023-04-25T16:20:23.928700Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"df_train['path'].values[3]","metadata":{"execution":{"iopub.status.busy":"2023-04-25T16:18:24.655583Z","iopub.execute_input":"2023-04-25T16:18:24.656046Z","iopub.status.idle":"2023-04-25T16:18:24.679831Z","shell.execute_reply.started":"2023-04-25T16:18:24.656008Z","shell.execute_reply":"2023-04-25T16:18:24.677951Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'train_landmark_files/25571/1000210073.parquet'"},"metadata":{}}]},{"cell_type":"code","source":"idx_plot = 3\npath_show = \"/kaggle/input/asl-signs/\"+df_train['path'].values[idx_plot]\nsign_plot = df_train['sign'].values[idx_plot]\npath_example = path_show.replace(\"_\", \"_\")\n\ndf = load_relevant_data(path_show)\ndf","metadata":{"execution":{"iopub.status.busy":"2023-04-25T16:20:29.009522Z","iopub.execute_input":"2023-04-25T16:20:29.009937Z","iopub.status.idle":"2023-04-25T16:20:29.268071Z","shell.execute_reply.started":"2023-04-25T16:20:29.009901Z","shell.execute_reply":"2023-04-25T16:20:29.266690Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"      frame            row_id        type  landmark_index         x         y  \\\n0        17         17-face-0        face               0  0.495870  0.478694   \n1        17         17-face-1        face               1  0.492222  0.447209   \n2        17         17-face-2        face               2  0.492067  0.457237   \n3        17         17-face-3        face               3  0.480419  0.415996   \n4        17         17-face-4        face               4  0.492035  0.437453   \n...     ...               ...         ...             ...       ...       ...   \n6511     28  28-right_hand-16  right_hand              16  0.506396  0.868416   \n6512     28  28-right_hand-17  right_hand              17  0.323227  0.835990   \n6513     28  28-right_hand-18  right_hand              18  0.435733  0.848917   \n6514     28  28-right_hand-19  right_hand              19  0.476093  0.867098   \n6515     28  28-right_hand-20  right_hand              20  0.488775  0.885244   \n\n             z  \n0    -0.037412  \n1    -0.067939  \n2    -0.035722  \n3    -0.050779  \n4    -0.072314  \n...        ...  \n6511 -0.139545  \n6512 -0.136632  \n6513 -0.156200  \n6514 -0.149442  \n6515 -0.142629  \n\n[6516 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>frame</th>\n      <th>row_id</th>\n      <th>type</th>\n      <th>landmark_index</th>\n      <th>x</th>\n      <th>y</th>\n      <th>z</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>17</td>\n      <td>17-face-0</td>\n      <td>face</td>\n      <td>0</td>\n      <td>0.495870</td>\n      <td>0.478694</td>\n      <td>-0.037412</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>17</td>\n      <td>17-face-1</td>\n      <td>face</td>\n      <td>1</td>\n      <td>0.492222</td>\n      <td>0.447209</td>\n      <td>-0.067939</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>17</td>\n      <td>17-face-2</td>\n      <td>face</td>\n      <td>2</td>\n      <td>0.492067</td>\n      <td>0.457237</td>\n      <td>-0.035722</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>17</td>\n      <td>17-face-3</td>\n      <td>face</td>\n      <td>3</td>\n      <td>0.480419</td>\n      <td>0.415996</td>\n      <td>-0.050779</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>17</td>\n      <td>17-face-4</td>\n      <td>face</td>\n      <td>4</td>\n      <td>0.492035</td>\n      <td>0.437453</td>\n      <td>-0.072314</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6511</th>\n      <td>28</td>\n      <td>28-right_hand-16</td>\n      <td>right_hand</td>\n      <td>16</td>\n      <td>0.506396</td>\n      <td>0.868416</td>\n      <td>-0.139545</td>\n    </tr>\n    <tr>\n      <th>6512</th>\n      <td>28</td>\n      <td>28-right_hand-17</td>\n      <td>right_hand</td>\n      <td>17</td>\n      <td>0.323227</td>\n      <td>0.835990</td>\n      <td>-0.136632</td>\n    </tr>\n    <tr>\n      <th>6513</th>\n      <td>28</td>\n      <td>28-right_hand-18</td>\n      <td>right_hand</td>\n      <td>18</td>\n      <td>0.435733</td>\n      <td>0.848917</td>\n      <td>-0.156200</td>\n    </tr>\n    <tr>\n      <th>6514</th>\n      <td>28</td>\n      <td>28-right_hand-19</td>\n      <td>right_hand</td>\n      <td>19</td>\n      <td>0.476093</td>\n      <td>0.867098</td>\n      <td>-0.149442</td>\n    </tr>\n    <tr>\n      <th>6515</th>\n      <td>28</td>\n      <td>28-right_hand-20</td>\n      <td>right_hand</td>\n      <td>20</td>\n      <td>0.488775</td>\n      <td>0.885244</td>\n      <td>-0.142629</td>\n    </tr>\n  </tbody>\n</table>\n<p>6516 rows Ã— 7 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#Let's explore one parquet file \nout=pd.read_parquet(\"/kaggle/input/asl-signs/\"+df_train['path'].values[1],columns = ['x', 'y', 'z']).values.reshape(11,543,3)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T16:35:05.629051Z","iopub.execute_input":"2023-04-25T16:35:05.629855Z","iopub.status.idle":"2023-04-25T16:35:05.644632Z","shell.execute_reply.started":"2023-04-25T16:35:05.629814Z","shell.execute_reply":"2023-04-25T16:35:05.643513Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"#No. of frames in file\nlen(out)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T16:35:08.720408Z","iopub.execute_input":"2023-04-25T16:35:08.721476Z","iopub.status.idle":"2023-04-25T16:35:08.728028Z","shell.execute_reply.started":"2023-04-25T16:35:08.721435Z","shell.execute_reply":"2023-04-25T16:35:08.726857Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"11"},"metadata":{}}]},{"cell_type":"code","source":"#No. of points in one frame\nlen(out[0])","metadata":{"execution":{"iopub.status.busy":"2023-04-25T16:35:11.462774Z","iopub.execute_input":"2023-04-25T16:35:11.463141Z","iopub.status.idle":"2023-04-25T16:35:11.469934Z","shell.execute_reply.started":"2023-04-25T16:35:11.463108Z","shell.execute_reply":"2023-04-25T16:35:11.468599Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"543"},"metadata":{}}]},{"cell_type":"code","source":"#No. of co-ordinates for one point\nlen(out[0][0])","metadata":{"execution":{"iopub.status.busy":"2023-04-25T16:35:15.891746Z","iopub.execute_input":"2023-04-25T16:35:15.892152Z","iopub.status.idle":"2023-04-25T16:35:15.899272Z","shell.execute_reply.started":"2023-04-25T16:35:15.892107Z","shell.execute_reply":"2023-04-25T16:35:15.897992Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"3"},"metadata":{}}]},{"cell_type":"code","source":"path_show = \"/kaggle/input/asl-signs/\"+df_train['path'].values[1]\nsign_show = df_train['sign'].values[1]\n\ndf_example = load_relevant_data_subset(path_show)\n\nframes = df_example.shape[0]\nkeypoints = df_example.shape[1]\nposition = df_example.shape[2]\n\nprint(\"\\nNumber of frames:\", frames)\nprint(\"Keypoints:\", keypoints)\nprint(\"X, Y Z postions:\", position)\nprint(\"Total number of datapoints in this sequence:\", np.prod(df_example.shape))\n\n\npose_landmarks = 33\nface_landmarks = 468\nright_hand_landmarks = 21\nstart_left_hand = face_landmarks\nleft_hand_landmarks = 21\nstart_right_hand = face_landmarks + left_hand_landmarks + pose_landmarks\ntotal_landmarks = pose_landmarks + face_landmarks + right_hand_landmarks + left_hand_landmarks\n\n\nprint(\"\\nPose landmarks:\", pose_landmarks)\nprint(\"Face landmarks:\", face_landmarks)\nprint(\"Right hand landmarks:\", right_hand_landmarks)\nprint(\"Left hand landmarks:\", left_hand_landmarks)\nprint(\"Total landmarks/keypoints: \", total_landmarks)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T17:09:42.850669Z","iopub.execute_input":"2023-04-25T17:09:42.851108Z","iopub.status.idle":"2023-04-25T17:09:42.875799Z","shell.execute_reply.started":"2023-04-25T17:09:42.851072Z","shell.execute_reply":"2023-04-25T17:09:42.874645Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"\nNumber of frames: 11\nKeypoints: 543\nX, Y Z postions: 3\nTotal number of datapoints in this sequence: 17919\n\nPose landmarks: 33\nFace landmarks: 468\nRight hand landmarks: 21\nLeft hand landmarks: 21\nTotal landmarks/keypoints:  543\n","output_type":"stream"}]},{"cell_type":"code","source":"from tqdm import tqdm\n\nmax_sequence_length = 32\nlip_marks = [61, 185, 40, 39, 37, 0, 267, 269, 270, 409, 291, 78, 191, 80, 81, 82, 13, 312, 311, 310, 415, 308, 95, 88, 178, 87, 14, 317, 402, 318, 324, 146, 91, 181, 84, 17, 314, 405, 321, 375]  \n\nlips = lip_marks\nleft_hand = [*range(start_left_hand, start_left_hand+left_hand_landmarks, 1)]\nright_hand = [*range(start_right_hand, start_right_hand+right_hand_landmarks, 1)]\nmeaningful_keypoints = lips + left_hand + right_hand\ninput_length = len(meaningful_keypoints)*3\n\ndef get_data(file_paths, y_sign):\n    \n    X = np.empty((file_paths.shape[0], max_sequence_length, len(meaningful_keypoints)*3), dtype=float)\n\n    for i in tqdm(range(file_paths.shape[0])):\n        file_name = \"/kaggle/input/asl-signs/\"+file_paths[i]\n        data = load_relevant_data_subset(file_name)\n        \n        data = data[:, meaningful_keypoints]\n        \n        if data.shape[0] < max_sequence_length:\n            rows = max_sequence_length - data.shape[0]\n            data = np.append(np.zeros((rows, len(meaningful_keypoints), 3)), data, axis=0)\n        elif data.shape[0] > max_sequence_length:\n            data = data[-(max_sequence_length):]\n\n        X[i] = data.reshape(max_sequence_length, len(meaningful_keypoints)*3, order='F')\n        \n        del data\n        \n    X = np.asarray(X).astype(np.float32)\n        \n    y = []\n    for sign in y_sign:\n        y.append(sign_dict[sign])\n\n    y = np.array(y, dtype=int)\n\n    return X, y","metadata":{"execution":{"iopub.status.busy":"2023-04-25T17:32:56.065282Z","iopub.execute_input":"2023-04-25T17:32:56.065975Z","iopub.status.idle":"2023-04-25T17:32:56.078982Z","shell.execute_reply.started":"2023-04-25T17:32:56.065938Z","shell.execute_reply":"2023-04-25T17:32:56.077748Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"**LSTM Training**","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import datasets, layers, models, Input, optimizers\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import pad_sequences","metadata":{"execution":{"iopub.status.busy":"2023-04-25T17:11:50.421789Z","iopub.execute_input":"2023-04-25T17:11:50.422192Z","iopub.status.idle":"2023-04-25T17:12:05.120000Z","shell.execute_reply.started":"2023-04-25T17:11:50.422159Z","shell.execute_reply":"2023-04-25T17:12:05.118831Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"def scaled_dot_product(q,k,v, softmax):\n    #calculates Q . K(transpose)\n    qkt = tf.matmul(q,k,transpose_b=True)\n    #caculates scaling factor\n    dk = tf.math.sqrt(tf.cast(q.shape[-1],dtype=tf.float32))\n    scaled_qkt = qkt/dk\n    softmax = softmax(scaled_qkt)\n    \n    z = tf.matmul(softmax,v)\n    #shape: (m,Tx,depth), same shape as q,k,v\n    return z\n\nclass MultiHeadAttention(tf.keras.layers.Layer):\n    def __init__(self,d_model,num_of_heads):\n        super(MultiHeadAttention,self).__init__()\n        self.d_model = d_model\n        self.num_of_heads = num_of_heads\n        self.depth = d_model//num_of_heads\n        self.wq = [tf.keras.layers.Dense(self.depth) for i in range(num_of_heads)]\n        self.wk = [tf.keras.layers.Dense(self.depth) for i in range(num_of_heads)]\n        self.wv = [tf.keras.layers.Dense(self.depth) for i in range(num_of_heads)]\n        self.wo = tf.keras.layers.Dense(d_model)\n        self.softmax = tf.keras.layers.Softmax()\n        \n    def call(self,x):\n        \n        multi_attn = []\n        for i in range(self.num_of_heads):\n            Q = self.wq[i](x)\n            K = self.wk[i](x)\n            V = self.wv[i](x)\n            multi_attn.append(scaled_dot_product(Q,K,V, self.softmax))\n            \n        multi_head = tf.concat(multi_attn,axis=-1)\n        multi_head_attention = self.wo(multi_head)\n        return multi_head_attention","metadata":{"execution":{"iopub.status.busy":"2023-04-25T17:32:34.789022Z","iopub.execute_input":"2023-04-25T17:32:34.789440Z","iopub.status.idle":"2023-04-25T17:32:34.802432Z","shell.execute_reply.started":"2023-04-25T17:32:34.789405Z","shell.execute_reply":"2023-04-25T17:32:34.801244Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"# a single dense block followed by a normalization block and relu activation\ndef dense_block(units):\n    fc = layers.Dense(units)\n    norm = layers.LayerNormalization()\n    act = layers.Activation(\"relu\")\n    drop = layers.Dropout(0.05)\n    return lambda x: drop(act(norm(fc(x))))\n\n# transformer blocks\ndef transformer_block(key_dim, x):\n    mha = MultiHeadAttention(key_dim, 8)(x)\n    add1 = layers.add([mha, x])\n    norm1 = layers.LayerNormalization()(add1)\n\n    fc = layers.Dense(key_dim, activation=\"relu\")(norm1)\n    add2 = tf.math.add(fc, norm1)\n    norm2 = layers.LayerNormalization()(add2)\n\n    return norm2\n\n# the final dense block for the classification\ndef classifier_lstm(units):\n    lstm = layers.LSTM(units)\n    out = layers.Dense(250, activation=\"softmax\", name=\"outputs\")\n    return lambda x: out(lstm(x))\n    \ndef classifier_transformer():\n    dense = layers.Dense(256, activation=\"relu\")\n    drop = layers.Dropout(0.1)\n    \n    out = layers.Dense(250, activation=\"softmax\", name=\"outputs\")\n    return lambda x: out(drop(dense(x)))\n\ninputs = tf.keras.Input(shape=(None, input_length), ragged=True)\n# choose the number of nodes per layer\nembedding_units = [256, 128, 256] # tune this\ntransformer_units = []#, 512, 512]\n\n# # dense encoder model\nx = inputs\nfor n in embedding_units:\n    x = dense_block(n)(x)\n    \nfor t in transformer_units:\n    x = transformer_block(t, x)\n\n# classifier layer\nif len(transformer_units) > 0:\n    # Pooling\n    x = tf.math.reduce_sum(x, axis=1)\n    out = classifier_transformer()(x)\nelse:\n    out = classifier_lstm(embedding_units[-1])(x)\n\n\nmodel = tf.keras.Model(inputs=inputs, outputs=out)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-04-25T17:33:05.050565Z","iopub.execute_input":"2023-04-25T17:33:05.052605Z","iopub.status.idle":"2023-04-25T17:33:11.023989Z","shell.execute_reply.started":"2023-04-25T17:33:05.052555Z","shell.execute_reply":"2023-04-25T17:33:11.023102Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"Model: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, None, 246)]       0         \n                                                                 \n dense (Dense)               (None, None, 256)         63232     \n                                                                 \n layer_normalization (LayerN  (None, None, 256)        512       \n ormalization)                                                   \n                                                                 \n activation (Activation)     (None, None, 256)         0         \n                                                                 \n dropout (Dropout)           (None, None, 256)         0         \n                                                                 \n dense_1 (Dense)             (None, None, 128)         32896     \n                                                                 \n layer_normalization_1 (Laye  (None, None, 128)        256       \n rNormalization)                                                 \n                                                                 \n activation_1 (Activation)   (None, None, 128)         0         \n                                                                 \n dropout_1 (Dropout)         (None, None, 128)         0         \n                                                                 \n dense_2 (Dense)             (None, None, 256)         33024     \n                                                                 \n layer_normalization_2 (Laye  (None, None, 256)        512       \n rNormalization)                                                 \n                                                                 \n activation_2 (Activation)   (None, None, 256)         0         \n                                                                 \n dropout_2 (Dropout)         (None, None, 256)         0         \n                                                                 \n lstm (LSTM)                 (None, 256)               525312    \n                                                                 \n outputs (Dense)             (None, 250)               64250     \n                                                                 \n=================================================================\nTotal params: 719,994\nTrainable params: 719,994\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"# add a decreasing learning rate scheduler to help convergence\nbatch_size = 256\nvalidation_percentage = 0.05\nsteps_per_epoch = int(94477*(1-validation_percentage)) // batch_size\nboundaries = [steps_per_epoch * n for n in [23, 35, 45, 53, 60]]\n# print(boundaries)\nvalues = [1e-3,1e-4,1e-5,1e-6,1e-7,1e-8]\nlr_sched = optimizers.schedules.PiecewiseConstantDecay(boundaries, values)\n\noptimizer = optimizers.Adam(lr_sched)\n# optimizer = optimizers.Adam()\n\nmodel.compile(optimizer=optimizer,\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(name=\"loss\"),\n              metrics=[\"accuracy\",\"sparse_top_k_categorical_accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2023-04-25T14:27:39.555015Z","iopub.execute_input":"2023-04-25T14:27:39.555820Z","iopub.status.idle":"2023-04-25T14:27:39.620047Z","shell.execute_reply.started":"2023-04-25T14:27:39.555779Z","shell.execute_reply":"2023-04-25T14:27:39.619083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_callbacks():\n    return [\n        tf.keras.callbacks.EarlyStopping(\n            monitor=\"val_accuracy\",\n            patience = 10,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor = \"val_accuracy\",\n            factor = 0.2,\n            patience = 5\n        ),\n    ]","metadata":{"execution":{"iopub.status.busy":"2023-04-25T14:27:43.360424Z","iopub.execute_input":"2023-04-25T14:27:43.361171Z","iopub.status.idle":"2023-04-25T14:27:43.366920Z","shell.execute_reply.started":"2023-04-25T14:27:43.361126Z","shell.execute_reply":"2023-04-25T14:27:43.365825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mode='training'","metadata":{"execution":{"iopub.status.busy":"2023-04-25T14:24:05.226583Z","iopub.execute_input":"2023-04-25T14:24:05.227296Z","iopub.status.idle":"2023-04-25T14:24:05.233413Z","shell.execute_reply.started":"2023-04-25T14:24:05.227257Z","shell.execute_reply":"2023-04-25T14:24:05.231999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if mode == \"training\":\n    file_paths = df_train['path'].values#[:1000]\n    y_sign = df_train['sign'].values#[:1000]\n    X, y = get_data(file_paths, y_sign)\n\n    X, X_val, y, y_val = train_test_split(X, y, test_size=validation_percentage, random_state=123)\n\n#     print(X.shape)\n#     print(y.shape)\n\n    history = model.fit(X, y, \n                    epochs=5,\n                    batch_size=batch_size,\n                    validation_data=(X_val, y_val),\n                    verbose=2,\n                    callbacks=[get_callbacks()]\n                   )\n\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    accuracy = history.history['accuracy']\n    val_accuracy = history.history['val_accuracy']","metadata":{"execution":{"iopub.status.busy":"2023-04-25T14:27:51.352212Z","iopub.execute_input":"2023-04-25T14:27:51.353369Z","iopub.status.idle":"2023-04-25T14:53:27.063341Z","shell.execute_reply.started":"2023-04-25T14:27:51.353319Z","shell.execute_reply":"2023-04-25T14:53:27.061195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if mode == \"training\":\n    import matplotlib.pyplot as plt\n    fig = plt.figure()\n\n    plt.plot(val_accuracy, label='val_accuracy')\n\n    plt.plot(accuracy, label='accuracy')\n\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    # plt.ylim([0.5, 1])\n    plt.legend()","metadata":{"execution":{"iopub.status.busy":"2023-04-25T14:56:19.006359Z","iopub.execute_input":"2023-04-25T14:56:19.007388Z","iopub.status.idle":"2023-04-25T14:56:19.310584Z","shell.execute_reply.started":"2023-04-25T14:56:19.007343Z","shell.execute_reply":"2023-04-25T14:56:19.309587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lips = lip_marks\nleft_hand = [*range(start_left_hand, start_left_hand+left_hand_landmarks, 1)]\nright_hand = [*range(start_right_hand, start_right_hand+right_hand_landmarks, 1)]\nmeaningful_keypoints = lips + left_hand + right_hand\n\n\ndef get_inference_model(model):\n    inputs = tf.keras.Input(shape=(ROWS_PER_FRAME,3), name=\"inputs\")\n    \n    # drop most of the face mesh\n    x = tf.gather(inputs, meaningful_keypoints, axis=1)\n\n    # fill nan\n    x = tf.where(tf.math.is_nan(x), tf.zeros_like(x), x)\n\n    # flatten landmark xyz coordinates ()\n    x = tf.concat([x[...,i] for i in range(3)], -1)\n\n    x = tf.expand_dims(x,0)\n    \n    # call trained model\n    out = model(x)\n    \n    # explicitly name the final (identity) layer for the submission format\n    outputs = layers.Activation(\"linear\", name=\"outputs\")(out)\n    \n    inference_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    inference_model.compile(loss=\"sparse_categorical_crossentropy\",\n                            metrics=\"accuracy\")\n    return inference_model","metadata":{"execution":{"iopub.status.busy":"2023-04-25T14:56:52.437821Z","iopub.execute_input":"2023-04-25T14:56:52.438220Z","iopub.status.idle":"2023-04-25T14:56:52.447570Z","shell.execute_reply.started":"2023-04-25T14:56:52.438184Z","shell.execute_reply":"2023-04-25T14:56:52.446498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inference_model = get_inference_model(model)\ninference_model.summary(expand_nested=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T14:57:30.331408Z","iopub.execute_input":"2023-04-25T14:57:30.331821Z","iopub.status.idle":"2023-04-25T14:57:30.786567Z","shell.execute_reply.started":"2023-04-25T14:57:30.331785Z","shell.execute_reply":"2023-04-25T14:57:30.785801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if mode == \"training\":\n    converter = tf.lite.TFLiteConverter.from_keras_model(inference_model)\n    tflite_model = converter.convert()","metadata":{"execution":{"iopub.status.busy":"2023-04-25T14:58:11.208250Z","iopub.execute_input":"2023-04-25T14:58:11.208975Z","iopub.status.idle":"2023-04-25T14:58:24.599297Z","shell.execute_reply.started":"2023-04-25T14:58:11.208934Z","shell.execute_reply":"2023-04-25T14:58:24.598167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if mode == \"training\":\n    with open('/kaggle/working/model.tflite', 'wb') as f:\n        f.write(tflite_model)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T15:05:36.726424Z","iopub.execute_input":"2023-04-25T15:05:36.727133Z","iopub.status.idle":"2023-04-25T15:05:36.737578Z","shell.execute_reply.started":"2023-04-25T15:05:36.727093Z","shell.execute_reply":"2023-04-25T15:05:36.736484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if mode == \"submission\":\n    !cp -r /kaggle/input/tflite-model/model.tflite ./","metadata":{"execution":{"iopub.status.busy":"2023-04-25T15:05:50.515721Z","iopub.execute_input":"2023-04-25T15:05:50.516747Z","iopub.status.idle":"2023-04-25T15:05:50.523238Z","shell.execute_reply.started":"2023-04-25T15:05:50.516701Z","shell.execute_reply":"2023-04-25T15:05:50.521968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip submission.zip model.tflite","metadata":{"execution":{"iopub.status.busy":"2023-04-25T15:06:08.931880Z","iopub.execute_input":"2023-04-25T15:06:08.932257Z","iopub.status.idle":"2023-04-25T15:06:10.162186Z","shell.execute_reply.started":"2023-04-25T15:06:08.932224Z","shell.execute_reply":"2023-04-25T15:06:10.160927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}